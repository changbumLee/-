{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a64296-a642-4f29-95c6-27606030609c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\Desktop\\matlab_ai\\Mask_RCNN\n"
     ]
    }
   ],
   "source": [
    "cd matlab_ai/Mask_RCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d84bba-b911-41d9-ac2f-ce6838d92b88",
   "metadata": {},
   "source": [
    "## 사용하는 코드) 최종"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ccbafc-1ad2-42a9-8d81-c5fe4b5a941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyttsx3\n",
    "pip install playsound\n",
    "pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb04978-c685-4b03-90f2-fa125b9ddbfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#찐최종\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib\n",
    "import cv2\n",
    "import pygame\n",
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "\n",
    "class_names = [\n",
    "    'bicycle', 'bus', 'car', 'carrier', 'cat', 'dog', 'motorcycle', \n",
    "    'movable_signage', 'person', 'scooter', 'stroller', 'truck', \n",
    "    'wheelchair', 'barricade', 'bench', 'bollard', 'chair', 'fire_hydrant', \n",
    "    'kiosk', 'parking_meter', 'pole', 'potted_plant', 'power_controller', \n",
    "    'stop', 'table', 'traffic_light', 'traffic_light_controller', \n",
    "    'traffic_sign', 'tree_trunk', 'caution_zone', 'sidewalk', \n",
    "    'roadway', 'braille_guide_blocks'\n",
    "]\n",
    "\n",
    "# Category mapping\n",
    "category_mapping = {\n",
    "    'bicycle': 'bicycle',\n",
    "    'bus': 'bus',\n",
    "    'car': 'vehicle',\n",
    "    'carrier': 'moving_obstacle',\n",
    "    'cat': 'moving_obstacle',\n",
    "    'dog': 'moving_obstacle',\n",
    "    'motorcycle': 'motorcycle',\n",
    "    'movable_signage': 'moving_obstacle',\n",
    "    'person': 'person',\n",
    "    'scooter': 'motorcycle',\n",
    "    'stroller': 'moving_obstacle',\n",
    "    'truck': 'vehicle',\n",
    "    'wheelchair': 'moving_obstacle',\n",
    "    'barricade': 'fixed_obstacle',\n",
    "    'bench': 'fixed_obstacle',\n",
    "    'bollard': 'fixed_obstacle',\n",
    "    'chair': 'fixed_obstacle',\n",
    "    'fire_hydrant': 'fixed_obstacle',\n",
    "    'kiosk': 'fixed_obstacle',\n",
    "    'parking_meter': 'fixed_obstacle',\n",
    "    'pole': 'pole',\n",
    "    'potted_plant': 'fixed_obstacle',\n",
    "    'power_controller': 'fixed_obstacle',\n",
    "    'stop': 'fixed_obstacle',\n",
    "    'table': 'fixed_obstacle',\n",
    "    'traffic_light': 'fixed_obstacle',\n",
    "    'traffic_light_controller': 'fixed_obstacle',\n",
    "    'traffic_sign': 'fixed_obstacle',\n",
    "    'tree_trunk': 'tree_trunk',\n",
    "    'caution_zone': 'caution_zone',\n",
    "    'sidewalk': 'sidewalk',\n",
    "    'roadway': 'roadway',\n",
    "    'braille_guide_blocks': 'braille_guide_blocks'\n",
    "}\n",
    "\n",
    "# Audio file paths\n",
    "audio_files = {\n",
    "    'bicycle': 'C:/Users/Home/Desktop/matlab_ai/audio/자전거.mp3',\n",
    "    'bus': 'C:/Users/Home/Desktop/matlab_ai/audio/버스.mp3',\n",
    "    'vehicle': 'C:/Users/Home/Desktop/matlab_ai/audio/차량.mp3',\n",
    "    'moving_obstacle': 'C:/Users/Home/Desktop/matlab_ai/audio/이동장애물.mp3',\n",
    "    'motorcycle': 'C:/Users/Home/Desktop/matlab_ai/audio/오토바이.mp3',\n",
    "    'fixed_obstacle': 'C:/Users/Home/Desktop/matlab_ai/audio/고정장애물.mp3',\n",
    "    'pole': 'C:/Users/Home/Desktop/matlab_ai/audio/기둥.mp3',\n",
    "    'tree_trunk': 'C:/Users/Home/Desktop/matlab_ai/audio/나무기둥.mp3',\n",
    "    'caution_zone': 'C:/Users/Home/Desktop/matlab_ai/audio/주의구역.mp3',\n",
    "    'sidewalk': 'C:/Users/Home/Desktop/matlab_ai/audio/인도.mp3',\n",
    "    'roadway': 'C:/Users/Home/Desktop/matlab_ai/audio/차도.mp3',\n",
    "    'braille_guide_blocks': 'C:/Users/Home/Desktop/matlab_ai/audio/점자블록.mp3',\n",
    "    'person': 'C:/Users/Home/Desktop/matlab_ai/audio/사람.mp3',\n",
    "    '전방에는': 'C:/Users/Home/Desktop/matlab_ai/audio/전방에는.mp3',\n",
    "    '좌측에는': 'C:/Users/Home/Desktop/matlab_ai/audio/좌측에는.mp3',\n",
    "    '우측에는': 'C:/Users/Home/Desktop/matlab_ai/audio/우측에는.mp3',\n",
    "    '전방에는 점자블록': 'C:/Users/Home/Desktop/matlab_ai/audio/전방에는 점자블록.mp3',\n",
    "    '좌측에는 점자블록': 'C:/Users/Home/Desktop/matlab_ai/audio/좌측에는 점자블록.mp3',\n",
    "    '우측에는 점자블록': 'C:/Users/Home/Desktop/matlab_ai/audio/우측에는 점자블록.mp3',\n",
    "    '전방에는 주의구역': 'C:/Users/Home/Desktop/matlab_ai/audio/전방에는 주의구역.mp3',\n",
    "    '좌측에는 주의구역': 'C:/Users/Home/Desktop/matlab_ai/audio/좌측에는 주의구역.mp3',\n",
    "    '우측에는 주의구역': 'C:/Users/Home/Desktop/matlab_ai/audio/우측에는 주의구역.mp3',\n",
    "}\n",
    "\n",
    "# Configuration class for Mask R-CNN\n",
    "class InferenceConfig(Config):\n",
    "    NAME = \"coco\"\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 33  # Number of classes including background\n",
    "    DETECTION_MIN_CONFIDENCE = 0.7  # General detection threshold\n",
    "\n",
    "config = InferenceConfig()\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=\"Mask_RCNN\", config=config)\n",
    "model.load_weights(r\"C:\\Users\\Home\\Desktop\\tm_mask_rcnn_test_0200.h5\", by_name=True)\n",
    "\n",
    "def calculate_area_and_center(mask):\n",
    "    area = np.sum(mask)\n",
    "    indices = np.where(mask)\n",
    "    if len(indices[0]) == 0 or len(indices[1]) == 0:\n",
    "        return 0, (0, 0)\n",
    "    y_center = np.mean(indices[0])\n",
    "    x_center = np.mean(indices[1])\n",
    "    return area, (int(x_center), int(y_center))\n",
    "\n",
    "def play_audio(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(file_path)\n",
    "        pygame.mixer.music.play()\n",
    "        while pygame.mixer.music.get_busy():\n",
    "            time.sleep(0.1)\n",
    "    else:\n",
    "        print(f\"Warning: Audio file {file_path} does not exist.\")\n",
    "\n",
    "def audio_player(audio_queue, interrupt_event):\n",
    "    while True:\n",
    "        audio_file = audio_queue.get()\n",
    "        if not interrupt_event.is_set():\n",
    "            play_audio(audio_file)\n",
    "        else:\n",
    "            while not audio_queue.empty():\n",
    "                audio_queue.get_nowait()\n",
    "            play_audio(audio_file)  # Play the interrupt message\n",
    "            interrupt_event.clear()\n",
    "        audio_queue.task_done()\n",
    "\n",
    "# GPU settings\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Desired size for the model input\n",
    "model_input_size = (800, 600)  # Adjusted size for better performance\n",
    "\n",
    "# Output screen size\n",
    "output_screen_size = (640, 480)  # Desired output screen size\n",
    "\n",
    "# Specific threshold for certain classes\n",
    "specific_thresholds = {\n",
    "    'sidewalk': 55.0,\n",
    "    'roadway': 55.0,\n",
    "    'caution_zone': 30.0,\n",
    "    'braille_guide_blocks': 30.0\n",
    "}\n",
    "\n",
    "# Audio queue and interrupt event\n",
    "audio_queue = Queue()\n",
    "interrupt_event = threading.Event()\n",
    "audio_thread = threading.Thread(target=audio_player, args=(audio_queue, interrupt_event), daemon=True)\n",
    "audio_thread.start()\n",
    "\n",
    "# Audio output control\n",
    "audio_duration = 7  # seconds\n",
    "last_audio_time = time.time()\n",
    "\n",
    "def is_within_box(center, box_top_left, box_bottom_right):\n",
    "    x, y = center\n",
    "    return (box_top_left[0] <= x <= box_bottom_right[0]) and (box_top_left[1] <= y <= box_bottom_right[1])\n",
    "\n",
    "def get_section(center, box_top_left, section_width):\n",
    "    x = center[0]\n",
    "    if x < box_top_left[0] + section_width:\n",
    "        return 'left'\n",
    "    elif x < box_top_left[0] + 2 * section_width:\n",
    "        return 'center'\n",
    "    else:\n",
    "        return 'right'\n",
    "\n",
    "def check_and_play_audio(audio_key):\n",
    "    global last_audio_time\n",
    "    current_time = time.time()\n",
    "    if current_time - last_audio_time >= audio_duration:\n",
    "        audio_file = audio_files.get(audio_key)\n",
    "        if audio_file:\n",
    "            audio_queue.put(audio_file)\n",
    "            last_audio_time = current_time\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run detection on the resized frame\n",
    "    resized_frame = cv2.resize(frame, model_input_size)\n",
    "    results = model.detect([resized_frame], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    detected_classes = {'center': set(), 'left': set(), 'right': set()}\n",
    "    \n",
    "    # Scale up bounding boxes and masks\n",
    "    for i in range(len(r['class_ids'])):\n",
    "        mask = r['masks'][:, :, i]\n",
    "        area, center = calculate_area_and_center(mask)\n",
    "        (x, y, w, h) = cv2.boundingRect(mask.astype(np.uint8))\n",
    "        class_id = r['class_ids'][i]\n",
    "        class_name = class_names[class_id - 1]\n",
    "        score = r['scores'][i]\n",
    "        score_percentage = int(score * 100)\n",
    "\n",
    "        # Determine the threshold based on class\n",
    "        threshold = specific_thresholds.get(category_mapping.get(class_name, class_name), config.DETECTION_MIN_CONFIDENCE * 100)\n",
    "\n",
    "        # Ensure no NaN values are used\n",
    "        if np.isnan(x) or np.isnan(y) or np.isnan(w) or np.isnan(h):\n",
    "            continue\n",
    "\n",
    "        # Determine if the object should be displayed based on its class and score\n",
    "        if score_percentage >= threshold:\n",
    "            # Scale up the coordinates to match the original frame size\n",
    "            scale_x = frame.shape[1] / model_input_size[0]\n",
    "            scale_y = frame.shape[0] / model_input_size[1]\n",
    "            x, y, w, h = int(x * scale_x), int(y * scale_y), int(w * scale_x), int(h * scale_y)\n",
    "            center = (int(center[0] * scale_x), int(center[1] * scale_y))\n",
    "\n",
    "            # Draw the bounding box and mask on the original frame\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.circle(frame, center, 5, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f'{class_name}: {score_percentage}%', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw the detection box and its sections\n",
    "            box_width, box_height = 600, 250\n",
    "            vertical_offset = 50\n",
    "            top_left = ((frame.shape[1] - box_width) // 2, (frame.shape[0] - box_height) // 2 + vertical_offset)\n",
    "            bottom_right = (top_left[0] + box_width, top_left[1] + box_height)\n",
    "            section_width = box_width // 3\n",
    "            cv2.rectangle(frame, top_left, bottom_right, (255, 0, 0), 2)\n",
    "            for i in range(3):\n",
    "                section_start = top_left[0] + i * section_width\n",
    "                section_end = section_start + section_width\n",
    "                cv2.line(frame, (section_start, top_left[1]), (section_start, bottom_right[1]), (0, 255, 255), 2)\n",
    "                cv2.line(frame, (section_end, top_left[1]), (section_end, bottom_right[1]), (0, 255, 255), 2)\n",
    "\n",
    "            # Check if the object's center is within the blue box\n",
    "            if is_within_box(center, top_left, bottom_right):\n",
    "                section = get_section(center, top_left, section_width)\n",
    "                detected_classes[section].add(class_name)\n",
    "\n",
    "    # Check for interrupt conditions\n",
    "    interrupt_detected = False\n",
    "    interrupt_message = None\n",
    "    for section in ['left', 'center', 'right']:\n",
    "        if 'caution_zone' in detected_classes[section]:\n",
    "            if section == 'left':\n",
    "                interrupt_message = '좌측에는 주의구역'\n",
    "            elif section == 'center':\n",
    "                interrupt_message = '전방에는 주의구역'\n",
    "            elif section == 'right':\n",
    "                interrupt_message = '우측에는 주의구역'\n",
    "            interrupt_detected = True\n",
    "            break\n",
    "        elif 'braille_guide_blocks' in detected_classes[section]:\n",
    "            if section == 'left':\n",
    "                interrupt_message = '좌측에는 점자블록'\n",
    "            elif section == 'center':\n",
    "                interrupt_message = '전방에는 점자블록'\n",
    "            elif section == 'right':\n",
    "                interrupt_message = '우측에는 점자블록'\n",
    "            interrupt_detected = True\n",
    "            break\n",
    "\n",
    "    if interrupt_detected:\n",
    "        interrupt_event.set()\n",
    "        audio_queue.put(audio_files.get(interrupt_message))\n",
    "        last_audio_time = time.time()\n",
    "    else:\n",
    "        current_time = time.time()\n",
    "        if current_time - last_audio_time >= audio_duration:\n",
    "            if detected_classes['center']:\n",
    "                audio_queue.put(audio_files['전방에는'])\n",
    "                for class_name in detected_classes['center']:\n",
    "                    audio_queue.put(audio_files.get(category_mapping.get(class_name, class_name)))\n",
    "            if detected_classes['left']:\n",
    "                audio_queue.put(audio_files['좌측에는'])\n",
    "                for class_name in detected_classes['left']:\n",
    "                    audio_queue.put(audio_files.get(category_mapping.get(class_name, class_name)))\n",
    "            if detected_classes['right']:\n",
    "                audio_queue.put(audio_files['우측에는'])\n",
    "                for class_name in detected_classes['right']:\n",
    "                    audio_queue.put(audio_files.get(category_mapping.get(class_name, class_name)))\n",
    "            last_audio_time = current_time\n",
    "\n",
    "    # Resize the frame to the output screen size\n",
    "    output_frame = cv2.resize(frame, output_screen_size)\n",
    "\n",
    "\n",
    "    # Show the resized frame\n",
    "    cv2.imshow('Mask R-CNN Detection', output_frame)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aea85e-ab4b-4b80-a600-d931b50249bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93451202-8193-479a-8e3a-93cefb219568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10f8113b-975e-4973-bbd9-28e093056f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#일단이건 킵 ( 나중에 사용 가능성 있음 )\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib\n",
    "import cv2\n",
    "import pygame\n",
    "import threading\n",
    "import time\n",
    "from queue import Queue\n",
    "\n",
    "class_names = [\n",
    "    'bicycle', 'bus', 'car', 'carrier', 'cat', 'dog', 'motorcycle', \n",
    "    'movable_signage', 'person', 'scooter', 'stroller', 'truck', \n",
    "    'wheelchair', 'barricade', 'bench', 'bollard', 'chair', 'fire_hydrant', \n",
    "    'kiosk', 'parking_meter', 'pole', 'potted_plant', 'power_controller', \n",
    "    'stop', 'table', 'traffic_light', 'traffic_light_controller', \n",
    "    'traffic_sign', 'tree_trunk', 'caution_zone', 'sidewalk', \n",
    "    'roadway', 'braille_guide_blocks'\n",
    "]\n",
    "\n",
    "# Category mapping\n",
    "category_mapping = {\n",
    "    'bicycle': 'bicycle',\n",
    "    'bus': 'bus',\n",
    "    'car': 'vehicle',\n",
    "    'carrier': 'moving_obstacle',\n",
    "    'cat': 'moving_obstacle',\n",
    "    'dog': 'moving_obstacle',\n",
    "    'motorcycle': 'motorcycle',\n",
    "    'movable_signage': 'moving_obstacle',\n",
    "    'person': 'person',\n",
    "    'scooter': 'motorcycle',\n",
    "    'stroller': 'moving_obstacle',\n",
    "    'truck': 'vehicle',\n",
    "    'wheelchair': 'moving_obstacle',\n",
    "    'barricade': 'fixed_obstacle',\n",
    "    'bench': 'fixed_obstacle',\n",
    "    'bollard': 'fixed_obstacle',\n",
    "    'chair': 'fixed_obstacle',\n",
    "    'fire_hydrant': 'fixed_obstacle',\n",
    "    'kiosk': 'fixed_obstacle',\n",
    "    'parking_meter': 'fixed_obstacle',\n",
    "    'pole': 'pole',\n",
    "    'potted_plant': 'fixed_obstacle',\n",
    "    'power_controller': 'fixed_obstacle',\n",
    "    'stop': 'fixed_obstacle',\n",
    "    'table': 'fixed_obstacle',\n",
    "    'traffic_light': 'fixed_obstacle',\n",
    "    'traffic_light_controller': 'fixed_obstacle',\n",
    "    'traffic_sign': 'fixed_obstacle',\n",
    "    'tree_trunk': 'tree_trunk',\n",
    "    'caution_zone': 'caution_zone',\n",
    "    'sidewalk': 'sidewalk',\n",
    "    'roadway': 'roadway',\n",
    "    'braille_guide_blocks': 'braille_guide_blocks'\n",
    "}\n",
    "\n",
    "# Audio file paths\n",
    "audio_files = {\n",
    "    'bicycle': 'C:/Users/Home/Desktop/matlab_ai/audio/자전거.mp3',\n",
    "    'bus': 'C:/Users/Home/Desktop/matlab_ai/audio/버스.mp3',\n",
    "    'vehicle': 'C:/Users/Home/Desktop/matlab_ai/audio/차량.mp3',\n",
    "    'moving_obstacle': 'C:/Users/Home/Desktop/matlab_ai/audio/이동장애물.mp3',\n",
    "    'motorcycle': 'C:/Users/Home/Desktop/matlab_ai/audio/오토바이.mp3',\n",
    "    'fixed_obstacle': 'C:/Users/Home/Desktop/matlab_ai/audio/고정장애물.mp3',\n",
    "    'pole': 'C:/Users/Home/Desktop/matlab_ai/audio/기둥.mp3',\n",
    "    'tree_trunk': 'C:/Users/Home/Desktop/matlab_ai/audio/나무기둥.mp3',\n",
    "    'caution_zone': 'C:/Users/Home/Desktop/matlab_ai/audio/주의구역.mp3',\n",
    "    'sidewalk': 'C:/Users/Home/Desktop/matlab_ai/audio/인도.mp3',\n",
    "    'roadway': 'C:/Users/Home/Desktop/matlab_ai/audio/차도.mp3',\n",
    "    'braille_guide_blocks': 'C:/Users/Home/Desktop/matlab_ai/audio/점자블록.mp3',\n",
    "    'person': 'C:/Users/Home/Desktop/matlab_ai/audio/사람.mp3',\n",
    "    '전방에는': 'C:/Users/Home/Desktop/matlab_ai/audio/전방에는.mp3',\n",
    "    '좌측에는': 'C:/Users/Home/Desktop/matlab_ai/audio/좌측에는.mp3',\n",
    "    '우측에는': 'C:/Users/Home/Desktop/matlab_ai/audio/우측에는.mp3'\n",
    "}\n",
    "\n",
    "# Configuration class for Mask R-CNN\n",
    "class InferenceConfig(Config):\n",
    "    NAME = \"coco\"\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 33  # Number of classes including background\n",
    "    DETECTION_MIN_CONFIDENCE = 0.65  # General detection threshold\n",
    "\n",
    "config = InferenceConfig()\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=\"Mask_RCNN\", config=config)\n",
    "model.load_weights(r\"C:\\Users\\Home\\Desktop\\tm_mask_rcnn_test_0100.h5\", by_name=True)\n",
    "\n",
    "def calculate_area_and_center(mask):\n",
    "    area = np.sum(mask)\n",
    "    indices = np.where(mask)\n",
    "    if len(indices[0]) == 0 or len(indices[1]) == 0:\n",
    "        return 0, (0, 0)\n",
    "    y_center = np.mean(indices[0])\n",
    "    x_center = np.mean(indices[1])\n",
    "    return area, (int(x_center), int(y_center))\n",
    "\n",
    "def play_audio(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(file_path)\n",
    "        pygame.mixer.music.play()\n",
    "        while pygame.mixer.music.get_busy():\n",
    "            time.sleep(0.1)\n",
    "    else:\n",
    "        print(f\"Warning: Audio file {file_path} does not exist.\")\n",
    "\n",
    "def audio_player(audio_queue):\n",
    "    while True:\n",
    "        audio_file = audio_queue.get()\n",
    "        play_audio(audio_file)\n",
    "        audio_queue.task_done()\n",
    "\n",
    "# GPU settings\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Desired size for the model input\n",
    "model_input_size = (800, 600)  # Adjusted size for better performance\n",
    "\n",
    "# Output screen size\n",
    "output_screen_size = (640, 480)  # Desired output screen size\n",
    "\n",
    "# Specific threshold for certain classes\n",
    "specific_thresholds = {\n",
    "    'sidewalk': 80.0,\n",
    "    'roadway': 80.0,\n",
    "    'caution_zone': 20.0,\n",
    "    'braille_guide_blocks': 20.0\n",
    "}\n",
    "\n",
    "# Audio queue\n",
    "audio_queue = Queue()\n",
    "audio_thread = threading.Thread(target=audio_player, args=(audio_queue,), daemon=True)\n",
    "audio_thread.start()\n",
    "\n",
    "# Previous detected class to avoid repetitive audio\n",
    "prev_detected_class = None\n",
    "\n",
    "# Audio output control\n",
    "audio_lock = threading.Lock()\n",
    "audio_duration = 8  # seconds\n",
    "last_audio_time = time.time()\n",
    "\n",
    "def is_within_box(center, box_top_left, box_bottom_right):\n",
    "    x, y = center\n",
    "    return (box_top_left[0] <= x <= box_bottom_right[0]) and (box_top_left[1] <= y <= box_bottom_right[1])\n",
    "\n",
    "def get_section(center, box_top_left, section_width):\n",
    "    x = center[0]\n",
    "    if x < box_top_left[0] + section_width:\n",
    "        return 'left'\n",
    "    elif x < box_top_left[0] + 2 * section_width:\n",
    "        return 'center'\n",
    "    else:\n",
    "        return 'right'\n",
    "\n",
    "def check_and_play_audio(audio_key):\n",
    "    global last_audio_time\n",
    "    current_time = time.time()\n",
    "    if current_time - last_audio_time >= audio_duration:\n",
    "        audio_file = audio_files.get(audio_key)\n",
    "        if audio_file:\n",
    "            audio_queue.put(audio_file)\n",
    "            last_audio_time = current_time\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Run detection on the resized frame\n",
    "    resized_frame = cv2.resize(frame, model_input_size)\n",
    "    results = model.detect([resized_frame], verbose=0)\n",
    "    r = results[0]\n",
    "\n",
    "    detected_classes = {'center': set(), 'left': set(), 'right': set()}\n",
    "    \n",
    "    # Scale up bounding boxes and masks\n",
    "    for i in range(len(r['class_ids'])):\n",
    "        mask = r['masks'][:, :, i]\n",
    "        area, center = calculate_area_and_center(mask)\n",
    "        (x, y, w, h) = cv2.boundingRect(mask.astype(np.uint8))\n",
    "        class_id = r['class_ids'][i]\n",
    "        class_name = class_names[class_id - 1]\n",
    "        score = r['scores'][i]\n",
    "        score_percentage = int(score * 100)\n",
    "\n",
    "        # Determine the threshold based on class\n",
    "        threshold = specific_thresholds.get(category_mapping.get(class_name, class_name), config.DETECTION_MIN_CONFIDENCE * 100)\n",
    "\n",
    "        # Ensure no NaN values are used\n",
    "        if np.isnan(x) or np.isnan(y) or np.isnan(w) or np.isnan(h):\n",
    "            continue\n",
    "\n",
    "        # Determine if the object should be displayed based on its class and score\n",
    "        if score_percentage >= threshold:\n",
    "            # Scale up the coordinates to match the original frame size\n",
    "            scale_x = frame.shape[1] / model_input_size[0]\n",
    "            scale_y = frame.shape[0] / model_input_size[1]\n",
    "            x, y, w, h = int(x * scale_x), int(y * scale_y), int(w * scale_x), int(h * scale_y)\n",
    "            center = (int(center[0] * scale_x), int(center[1] * scale_y))\n",
    "\n",
    "            # Draw the bounding box and mask on the original frame\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.circle(frame, center, 5, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f'{class_name}: {score_percentage}%', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw the detection box and its sections\n",
    "            box_width, box_height = 600, 250\n",
    "            vertical_offset = 50\n",
    "            top_left = ((frame.shape[1] - box_width) // 2, (frame.shape[0] - box_height) // 2 + vertical_offset)\n",
    "            bottom_right = (top_left[0] + box_width, top_left[1] + box_height)\n",
    "            section_width = box_width // 3\n",
    "            cv2.rectangle(frame, top_left, bottom_right, (255, 0, 0), 2)\n",
    "            for i in range(3):\n",
    "                section_start = top_left[0] + i * section_width\n",
    "                section_end = section_start + section_width\n",
    "                cv2.line(frame, (section_start, top_left[1]), (section_start, bottom_right[1]), (0, 255, 255), 2)\n",
    "                cv2.line(frame, (section_end, top_left[1]), (section_end, bottom_right[1]), (0, 255, 255), 2)\n",
    "\n",
    "            # Check if the object's center is within the blue box\n",
    "            if is_within_box(center, top_left, bottom_right):\n",
    "                section = get_section(center, top_left, section_width)\n",
    "                detected_classes[section].add(class_name)\n",
    "\n",
    "    current_time = time.time()\n",
    "    if current_time - last_audio_time >= audio_duration:\n",
    "        if detected_classes['center']:\n",
    "            audio_queue.put(audio_files['전방에는'])\n",
    "            for class_name in detected_classes['center']:\n",
    "                audio_queue.put(audio_files.get(category_mapping.get(class_name, class_name)))\n",
    "        if detected_classes['left']:\n",
    "            audio_queue.put(audio_files['좌측에는'])\n",
    "            for class_name in detected_classes['left']:\n",
    "                audio_queue.put(audio_files.get(category_mapping.get(class_name, class_name)))\n",
    "        if detected_classes['right']:\n",
    "            audio_queue.put(audio_files['우측에는'])\n",
    "            for class_name in detected_classes['right']:\n",
    "                audio_queue.put(audio_files.get(category_mapping.get(class_name, class_name)))\n",
    "        last_audio_time = current_time\n",
    "\n",
    "    # Resize the frame to the output screen size\n",
    "    output_frame = cv2.resize(frame, output_screen_size)\n",
    "\n",
    "    # Show the resized frame\n",
    "    cv2.imshow('Mask R-CNN Detection', output_frame)\n",
    "\n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0f78a-9b45-49a6-97df-82534507ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import Label\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import time\n",
    "import numpy as np\n",
    "import pygame\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib\n",
    "\n",
    "class_names = [\n",
    "    'bicycle', 'bus', 'car', 'carrier', 'cat', 'dog', 'motorcycle', \n",
    "    'movable_signage', 'person', 'scooter', 'stroller', 'truck', \n",
    "    'wheelchair', 'barricade', 'bench', 'bollard', 'chair', 'fire_hydrant', \n",
    "    'kiosk', 'parking_meter', 'pole', 'potted_plant', 'power_controller', \n",
    "    'stop', 'table', 'traffic_light', 'traffic_light_controller', \n",
    "    'traffic_sign', 'tree_trunk', 'caution_zone', 'sidewalk', \n",
    "    'roadway', 'braille_guide_blocks'\n",
    "]\n",
    "\n",
    "audio_files = {\n",
    "    'bicycle': 'C:/Users/Home/Desktop/matlab_ai/audio/자전거.mp3',\n",
    "    'bus': 'C:/Users/Home/Desktop/matlab_ai/audio/버스.mp3',\n",
    "    'vehicle': 'C:/Users/Home/Desktop/matlab_ai/audio/차량.mp3',\n",
    "    'moving_obstacle': 'C:/Users/Home/Desktop/matlab_ai/audio/이동장애물.mp3',\n",
    "    'motorcycle': 'C:/Users/Home/Desktop/matlab_ai/audio/오토바이.mp3',\n",
    "    'fixed_obstacle': 'C:/Users/Home/Desktop/matlab_ai/audio/고정장애물.mp3',\n",
    "    'pole': 'C:/Users/Home/Desktop/matlab_ai/audio/기둥.mp3',\n",
    "    'tree_trunk': 'C:/Users/Home/Desktop/matlab_ai/audio/나무기둥.mp3',\n",
    "    'caution_zone': 'C:/Users/Home/Desktop/matlab_ai/audio/주의구역.mp3',\n",
    "    'sidewalk': 'C:/Users/Home/Desktop/matlab_ai/audio/인도.mp3',\n",
    "    'roadway': 'C:/Users/Home/Desktop/matlab_ai/audio/차도.mp3',\n",
    "    'braille_guide_blocks': 'C:/Users/Home/Desktop/matlab_ai/audio/점자블록.mp3',\n",
    "    'person': 'C:/Users/Home/Desktop/matlab_ai/audio/사람.mp3',\n",
    "    '전방에는': 'C:/Users/Home/Desktop/matlab_ai/audio/전방에는.mp3',\n",
    "    '좌측에는': 'C:/Users/Home/Desktop/matlab_ai/audio/좌측에는.mp3',\n",
    "    '우측에는': 'C:/Users/Home/Desktop/matlab_ai/audio/우측에는.mp3',\n",
    "    '전방에는 점자블록': 'C:/Users/Home/Desktop/matlab_ai/audio/전방에는 점자블록.mp3',\n",
    "    '좌측에는 점자블록': 'C:/Users/Home/Desktop/matlab_ai/audio/좌측에는 점자블록.mp3',\n",
    "    '우측에는 점자블록': 'C:/Users/Home/Desktop/matlab_ai/audio/우측에는 점자블록.mp3',\n",
    "    '전방에는 주의구역': 'C:/Users/Home/Desktop/matlab_ai/audio/전방에는 주의구역.mp3',\n",
    "    '좌측에는 주의구역': 'C:/Users/Home/Desktop/matlab_ai/audio/좌측에는 주의구역.mp3',\n",
    "    '우측에는 주의구역': 'C:/Users/Home/Desktop/matlab_ai/audio/우측에는 주의구역.mp3',\n",
    "}\n",
    "\n",
    "# Configuration class for Mask R-CNN\n",
    "class InferenceConfig(Config):\n",
    "    NAME = \"coco\"\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    NUM_CLASSES = 1 + 33  # Number of classes including background\n",
    "    DETECTION_MIN_CONFIDENCE = 0.6  # General detection threshold\n",
    "\n",
    "config = InferenceConfig()\n",
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=\"Mask_RCNN\", config=config)\n",
    "model.load_weights(r\"C:\\Users\\Home\\Desktop\\tm_mask_rcnn_test_0100.h5\", by_name=True)\n",
    "\n",
    "def calculate_area_and_center(mask):\n",
    "    area = np.sum(mask)\n",
    "    indices = np.where(mask)\n",
    "    if len(indices[0]) == 0 or len(indices[1]) == 0:\n",
    "        return 0, (0, 0)\n",
    "    y_center = np.mean(indices[0])\n",
    "    x_center = np.mean(indices[1])\n",
    "    return area, (int(x_center), int(y_center))\n",
    "\n",
    "def play_audio(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        pygame.mixer.init()\n",
    "        pygame.mixer.music.load(file_path)\n",
    "        pygame.mixer.music.play()\n",
    "        while pygame.mixer.music.get_busy():\n",
    "            time.sleep(0.1)\n",
    "    else:\n",
    "        print(f\"Warning: Audio file {file_path} does not exist.\")\n",
    "\n",
    "def audio_player(audio_queue, interrupt_event):\n",
    "    while True:\n",
    "        audio_file = audio_queue.get()\n",
    "        if not interrupt_event.is_set():\n",
    "            play_audio(audio_file)\n",
    "        else:\n",
    "            while not audio_queue.empty():\n",
    "                audio_queue.get_nowait()\n",
    "            play_audio(audio_file)  # Play the interrupt message\n",
    "            interrupt_event.clear()\n",
    "        audio_queue.task_done()\n",
    "\n",
    "# GPU settings\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "class App:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Object Detection and Audio Output\")\n",
    "\n",
    "        # Create video display label\n",
    "        self.video_label = Label(root)\n",
    "        self.video_label.pack()\n",
    "\n",
    "        # Create status label\n",
    "        self.status_label = Label(root, text=\"Status: Waiting...\")\n",
    "        self.status_label.pack()\n",
    "\n",
    "        # Initialize video capture\n",
    "        self.cap = cv2.VideoCapture(1)\n",
    "\n",
    "        # Initialize audio queue and event\n",
    "        self.audio_queue = Queue()\n",
    "        self.interrupt_event = threading.Event()\n",
    "        self.audio_thread = threading.Thread(target=audio_player, args=(self.audio_queue, self.interrupt_event), daemon=True)\n",
    "        self.audio_thread.start()\n",
    "\n",
    "        # Start video update thread\n",
    "        self.video_thread = threading.Thread(target=self.update_video)\n",
    "        self.video_thread.daemon = True\n",
    "        self.video_thread.start()\n",
    "\n",
    "        self.last_audio_time = time.time()\n",
    "        self.audio_duration = 7  # seconds\n",
    "\n",
    "    def update_video(self):\n",
    "        while True:\n",
    "            ret, frame = self.cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "\n",
    "            # Run detection on the resized frame\n",
    "            model_input_size = (800, 600)\n",
    "            resized_frame = cv2.resize(frame, model_input_size)\n",
    "            results = model.detect([resized_frame], verbose=0)\n",
    "            r = results[0]\n",
    "\n",
    "            detected_classes = {'center': set(), 'left': set(), 'right': set()}\n",
    "\n",
    "            # Scale up bounding boxes and masks\n",
    "            for i in range(len(r['class_ids'])):\n",
    "                mask = r['masks'][:, :, i]\n",
    "                area, center = calculate_area_and_center(mask)\n",
    "                (x, y, w, h) = cv2.boundingRect(mask.astype(np.uint8))\n",
    "                class_id = r['class_ids'][i]\n",
    "                class_name = class_names[class_id - 1]\n",
    "                score = r['scores'][i]\n",
    "                score_percentage = int(score * 100)\n",
    "\n",
    "                # Determine the threshold based on class\n",
    "                threshold = specific_thresholds.get(category_mapping.get(class_name, class_name), config.DETECTION_MIN_CONFIDENCE * 100)\n",
    "\n",
    "                # Ensure no NaN values are used\n",
    "                if np.isnan(x) or np.isnan(y) or np.isnan(w) or np.isnan(h):\n",
    "                    continue\n",
    "\n",
    "                # Determine if the object should be displayed based on its class and score\n",
    "                if score_percentage >= threshold:\n",
    "                    # Scale up the coordinates to match the original frame size\n",
    "                    scale_x = frame.shape[1] / model_input_size[0]\n",
    "                    scale_y = frame.shape[0] / model_input_size[1]\n",
    "                    x, y, w, h = int(x * scale_x), int(y * scale_y), int(w * scale_x), int(h * scale_y)\n",
    "                    center = (int(center[0] * scale_x), int(center[1] * scale_y))\n",
    "\n",
    "                    # Draw the bounding box and mask on the original frame\n",
    "                    cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                    cv2.circle(frame, center, 5, (0, 0, 255), -1)\n",
    "                    cv2.putText(frame, f'{class_name}: {score_percentage}%', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "                    # Draw the detection box and its sections\n",
    "                    box_width, box_height = w, h\n",
    "                    detection_area = {\n",
    "                        'center': (x + box_width // 2, y + box_height // 2),\n",
    "                        'left': (x + box_width // 4, y + box_height // 2),\n",
    "                        'right': (x + 3 * box_width // 4, y + box_height // 2)\n",
    "                    }\n",
    "                    \n",
    "                    # Determine position relative to the center\n",
    "                    if detection_area['center'][0] < frame.shape[1] // 3:\n",
    "                        detected_classes['left'].add(class_name)\n",
    "                    elif detection_area['center'][0] > 2 * frame.shape[1] // 3:\n",
    "                        detected_classes['right'].add(class_name)\n",
    "                    else:\n",
    "                        detected_classes['center'].add(class_name)\n",
    "\n",
    "            # Determine if the audio should be played\n",
    "            current_time = time.time()\n",
    "            if current_time - self.last_audio_time > self.audio_duration:\n",
    "                self.last_audio_time = current_time\n",
    "                self.interrupt_event.set()\n",
    "                self.audio_queue.put(audio_files.get('전방에는', ''))\n",
    "\n",
    "                for position in ['center', 'left', 'right']:\n",
    "                    for class_name in detected_classes[position]:\n",
    "                        audio_file = audio_files.get(class_name, '')\n",
    "                        if audio_file:\n",
    "                            self.audio_queue.put(audio_file)\n",
    "                            self.interrupt_event.clear()\n",
    "\n",
    "            # Convert frame to ImageTk format\n",
    "            image = Image.fromarray(frame)\n",
    "            imgtk = ImageTk.PhotoImage(image=image)\n",
    "            self.video_label.imgtk = imgtk\n",
    "            self.video_label.configure(image=imgtk)\n",
    "\n",
    "            # Update status\n",
    "            self.status_label.configure(text=\"Status: Processing...\")\n",
    "\n",
    "            self.root.update_idletasks()\n",
    "            self.root.update()\n",
    "\n",
    "    def play_audio(self, file_path):\n",
    "        if os.path.exists(file_path):\n",
    "            pygame.mixer.init()\n",
    "            pygame.mixer.music.load(file_path)\n",
    "            pygame.mixer.music.play()\n",
    "            while pygame.mixer.music.get_busy():\n",
    "                time.sleep(0.1)\n",
    "        else:\n",
    "            print(f\"Warning: Audio file {file_path} does not exist.\")\n",
    "\n",
    "# Create and run the application\n",
    "root = tk.Tk()\n",
    "app = App(root)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb68ef6-604d-4a86-9a17-87fe8769329e",
   "metadata": {},
   "source": [
    "## 사용하는 코드) TTS 사용하여 MP3 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0747cad-e1c0-4e79-bf65-8e794936e9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio file \"전방에는 주의구역.mp3\" created successfully.\n",
      "Audio file \"좌측에는 주의구역.mp3\" created successfully.\n",
      "Audio file \"우측에는 주의구역.mp3\" created successfully.\n",
      "Audio file \"전방에는 점자블록.mp3\" created successfully.\n",
      "Audio file \"좌측에는 점자블록.mp3\" created successfully.\n",
      "Audio file \"우측에는 점자블록.mp3\" created successfully.\n"
     ]
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# 음성 파일 저장할 폴더 경로\n",
    "audio_files_folder = r\"C:\\Users\\Home\\Desktop\\matlab_ai\\audio\"\n",
    "\n",
    "# 음성 파일 생성 함수\n",
    "def create_audio_file(text, filename):\n",
    "    tts = gTTS(text=text, lang='ko')  # 한국어로 설정\n",
    "    tts.save(os.path.join(audio_files_folder, filename))\n",
    "    print(f'Audio file \"{filename}\" created successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6806b7-3dea-482a-aff3-47679fef7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음성 파일 생성\n",
    "create_audio_file('전방에는 주의구역', '전방에는 주의구역.mp3')\n",
    "create_audio_file('좌측에는 주의구역', '좌측에는 주의구역.mp3')\n",
    "create_audio_file('우측에는 주의구역', '우측에는 주의구역.mp3')\n",
    "create_audio_file('전방에는 점자블록', '전방에는 점자블록.mp3')\n",
    "create_audio_file('좌측에는 점자블록', '좌측에는 점자블록.mp3')\n",
    "create_audio_file('우측에는 점자블록', '우측에는 점자블록.mp3')\n",
    "create_audio_file('전방에는', '전방에는.mp3')\n",
    "create_audio_file('좌측에는', '좌측에는.mp3')\n",
    "create_audio_file('우측에는', '우측에는.mp3')\n",
    "create_audio_file('자전거', '자전거.mp3')\n",
    "create_audio_file('버스', '버스.mp3')\n",
    "create_audio_file('차량', '차량.mp3')\n",
    "create_audio_file('이동장애물', '이동장애물.mp3')\n",
    "create_audio_file('오토바이', '오토바이.mp3')\n",
    "create_audio_file('고정장애물', '고정장애물.mp3')\n",
    "create_audio_file('기둥', '기둥.mp3')\n",
    "create_audio_file('나무기둥', '나무기둥.mp3')\n",
    "create_audio_file('주의구역', '주의구역.mp3')\n",
    "create_audio_file('인도', '인도.mp3')\n",
    "create_audio_file('차도', '차도.mp3')\n",
    "create_audio_file('점자블록', '점자블록.mp3')\n",
    "create_audio_file('사람', '사람.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd369e-b50a-4f76-8b6a-cc879db3fc98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "410c8a38-991a-4975-9cf9-8196e0c83ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-texttospeech in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (2.16.5)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2.19.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from google-cloud-texttospeech) (2.32.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from google-cloud-texttospeech) (1.24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from google-cloud-texttospeech) (5.27.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.63.2)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.65.2)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (1.65.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-texttospeech) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\home\\anaconda3\\envs\\2.5\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-texttospeech) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade google-cloud-texttospeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5488a6b2-d8d0-4ff3-86c7-fa4e454b248d",
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m         out\u001b[38;5;241m.\u001b[39mwrite(audio_data)\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m오디오 파일 생성\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[43msynthesize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m안녕하세요. 팀노바 티스토리 블로그에 오신 것을 환영합니다.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36msynthesize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msynthesize_text\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# \"texttospeech import\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m texttospeech        \n\u001b[1;32m----> 5\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43mtexttospeech\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextToSpeechClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# 최대 길이를 200으로 지정 (지나치게 길어지면 에러 발생)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     max_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m    \n",
      "File \u001b[1;32m~\\anaconda3\\envs\\2.5\\lib\\site-packages\\google\\cloud\\texttospeech_v1\\services\\text_to_speech\\client.py:667\u001b[0m, in \u001b[0;36mTextToSpeechClient.__init__\u001b[1;34m(self, credentials, transport, client_options, client_info)\u001b[0m\n\u001b[0;32m    659\u001b[0m transport_init: Union[\n\u001b[0;32m    660\u001b[0m     Type[TextToSpeechTransport], Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, TextToSpeechTransport]\n\u001b[0;32m    661\u001b[0m ] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, TextToSpeechTransport], transport)\n\u001b[0;32m    665\u001b[0m )\n\u001b[0;32m    666\u001b[0m \u001b[38;5;66;03m# initialize with the provided callable or the passed in class\u001b[39;00m\n\u001b[1;32m--> 667\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport \u001b[38;5;241m=\u001b[39m \u001b[43mtransport_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_cert_source_for_mtls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_cert_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\2.5\\lib\\site-packages\\google\\cloud\\texttospeech_v1\\services\\text_to_speech\\transports\\grpc.py:153\u001b[0m, in \u001b[0;36mTextToSpeechGrpcTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_channel_credentials \u001b[38;5;241m=\u001b[39m grpc\u001b[38;5;241m.\u001b[39mssl_channel_credentials(\n\u001b[0;32m    149\u001b[0m                 certificate_chain\u001b[38;5;241m=\u001b[39mcert, private_key\u001b[38;5;241m=\u001b[39mkey\n\u001b[0;32m    150\u001b[0m             )\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# The base transport sets the host, credentials and scopes\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grpc_channel:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# initialize with the provided callable or the default channel\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     channel_init \u001b[38;5;241m=\u001b[39m channel \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreate_channel\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\2.5\\lib\\site-packages\\google\\cloud\\texttospeech_v1\\services\\text_to_speech\\transports\\base.py:100\u001b[0m, in \u001b[0;36mTextToSpeechTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mload_credentials_from_file(\n\u001b[0;32m     97\u001b[0m         credentials_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscopes_kwargs, quota_project_id\u001b[38;5;241m=\u001b[39mquota_project_id\n\u001b[0;32m     98\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_credentials:\n\u001b[1;32m--> 100\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# Don't apply audience if the credentials file passed from user.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(credentials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_gdch_audience\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\2.5\\lib\\site-packages\\google\\auth\\_default.py:691\u001b[0m, in \u001b[0;36mdefault\u001b[1;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[0;32m    683\u001b[0m             _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    684\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo project ID could be determined. Consider running \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    685\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    686\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    687\u001b[0m                 environment_vars\u001b[38;5;241m.\u001b[39mPROJECT,\n\u001b[0;32m    688\u001b[0m             )\n\u001b[0;32m    689\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[1;32m--> 691\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "def synthesize_text(text):\n",
    "    # \"texttospeech import\"\n",
    "    from google.cloud import texttospeech        \n",
    "\n",
    "    client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "    # 최대 길이를 200으로 지정 (지나치게 길어지면 에러 발생)\n",
    "    max_length = 200    \n",
    "    # . 단위로 문장 분리\n",
    "    words = text.split('. ')\n",
    "    sentences = []\n",
    "    current_sentence = ''\n",
    "    for word in words:\n",
    "        if len(current_sentence + word) <= max_length:\n",
    "            current_sentence += word + ' '\n",
    "        else:\n",
    "            sentences.append(current_sentence.strip() + '.')\n",
    "            current_sentence = word + ' '\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence.strip() + '.')\n",
    "\n",
    "    \n",
    "\n",
    "    # 빈 배열 생성\n",
    "    audio_data = []\n",
    "\n",
    "    # 문장 개수 단위로 텍스트 변환\n",
    "    for sentence in sentences:\n",
    "        input_text = texttospeech.SynthesisInput(text=sentence)\n",
    "\n",
    "        # 오디오 설정 (예제에서는 한국어, 남성C)\n",
    "        voice = texttospeech.VoiceSelectionParams(\n",
    "            language_code=\"ko-KR\",\n",
    "            name=\"ko-KR-Neural2-C\",\n",
    "            ssml_gender=texttospeech.SsmlVoiceGender.MALE,\n",
    "        )\n",
    "\n",
    "        audio_config = texttospeech.AudioConfig(\n",
    "            audio_encoding=texttospeech.AudioEncoding.MP3\n",
    "        )\n",
    "\n",
    "        response = client.synthesize_speech(\n",
    "            request={\"input\": input_text, \"voice\": voice, \"audio_config\": audio_config}\n",
    "        )\n",
    "\n",
    "        audio_data.append(response.audio_content)\n",
    "  \n",
    "    audio_data = b\"\".join(audio_data)\n",
    "    \n",
    "    # audio 폴더 안에 output.mp3라는 이름으로 파일 생성\n",
    "    with open(\"audio/output.mp3\", \"wb\") as out:        \n",
    "        out.write(audio_data)\n",
    "        print('오디오 파일 생성')\n",
    "\n",
    "\n",
    "synthesize_text(\"안녕하세요. 팀노바 티스토리 블로그에 오신 것을 환영합니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_2.5",
   "language": "python",
   "name": "2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
